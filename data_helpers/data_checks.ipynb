{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRS Data Checks\n",
    "Quick stats for CSVs and folders of cleaned CRS data.\n",
    "\n",
    "- Tokens in a single CSV (sum; uses tiktoken if available, else words)\n",
    "- Number of reports (rows) in a folder of clean_*.csv\n",
    "- Number above a minimum word threshold\n",
    "- Total token estimate across the folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, glob, json\n",
    "from typing import List\n",
    "\n",
    "# Paths and knobs â€” edit these:\n",
    "csv_path = '../data/firstN/ai_first_10000__gemini-2.5-flash-lite.csv'  # a single CSV\n",
    "folder = '../data/1'                                 # folder with clean_*.csv files\n",
    "min_words = 3000                                   # threshold for 'above min'\n",
    "tokenizer_hint = 'gpt-4o-mini'                    # tiktoken model hint if installed\n",
    "\n",
    "# Try to import tiktoken (optional)\n",
    "try:\n",
    "    import tiktoken  # type: ignore\n",
    "except Exception:\n",
    "    tiktoken = None\n",
    "\n",
    "def get_tokenizer(model_hint: str):\n",
    "    if tiktoken is not None:\n",
    "        try:\n",
    "            enc = tiktoken.encoding_for_model(model_hint)\n",
    "            return lambda s: len(enc.encode(s or ''))\n",
    "        except Exception:\n",
    "            try:\n",
    "                enc = tiktoken.get_encoding('cl100k_base')\n",
    "                return lambda s: len(enc.encode(s or ''))\n",
    "            except Exception:\n",
    "                pass\n",
    "    return lambda s: len((s or '').split())\n",
    "\n",
    "tok = get_tokenizer(tokenizer_hint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens in a single CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"csv_path\": \"../data/firstN/ai_first_10000__gemini-2.5-flash-lite.csv\",\n",
      "  \"rows\": 1,\n",
      "  \"tokens\": 10000\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "single_tokens = 0\n",
    "single_rows = 0\n",
    "if os.path.isfile(csv_path):\n",
    "    with open(csv_path, 'r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            single_rows += 1\n",
    "            text = row.get('text') or ''\n",
    "            # if token_est available, prefer it; else compute\n",
    "            token_est = row.get('token_est')\n",
    "            try:\n",
    "                single_tokens += int(token_est) if token_est is not None else tok(text)\n",
    "            except Exception:\n",
    "                single_tokens += tok(text)\n",
    "else:\n",
    "    print(f'CSV not found: {csv_path}')\n",
    "\n",
    "print(json.dumps({'csv_path': csv_path, 'rows': single_rows, 'tokens': int(single_tokens)}, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder stats (clean_*.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"folder\": \"../data/1\",\n",
      "  \"files\": 8,\n",
      "  \"rows\": 263,\n",
      "  \"above_min\": 3216,\n",
      "  \"tokens\": 1518830\n",
      "}\n",
      "Per-file breakdown (first 10):\n",
      "{\n",
      "  \"file\": \"../data/1/clean_0.csv\",\n",
      "  \"rows\": 2,\n",
      "  \"above_min\": 2,\n",
      "  \"tokens\": 17783\n",
      "}\n",
      "{\n",
      "  \"file\": \"../data/1/clean_1.csv\",\n",
      "  \"rows\": 50,\n",
      "  \"above_min\": 3,\n",
      "  \"tokens\": 14499\n",
      "}\n",
      "{\n",
      "  \"file\": \"../data/1/clean_17.csv\",\n",
      "  \"rows\": 26,\n",
      "  \"above_min\": 26,\n",
      "  \"tokens\": 189842\n",
      "}\n",
      "{\n",
      "  \"file\": \"../data/1/clean_30.csv\",\n",
      "  \"rows\": 47,\n",
      "  \"above_min\": 47,\n",
      "  \"tokens\": 339773\n",
      "}\n",
      "{\n",
      "  \"file\": \"../data/1/clean_31.csv\",\n",
      "  \"rows\": 50,\n",
      "  \"above_min\": 50,\n",
      "  \"tokens\": 365087\n",
      "}\n",
      "{\n",
      "  \"file\": \"../data/1/clean_41.csv\",\n",
      "  \"rows\": 15,\n",
      "  \"above_min\": 15,\n",
      "  \"tokens\": 100675\n",
      "}\n",
      "{\n",
      "  \"file\": \"../data/1/clean_50.csv\",\n",
      "  \"rows\": 30,\n",
      "  \"above_min\": 30,\n",
      "  \"tokens\": 200225\n",
      "}\n",
      "{\n",
      "  \"file\": \"../data/1/clean_57.csv\",\n",
      "  \"rows\": 43,\n",
      "  \"above_min\": 43,\n",
      "  \"tokens\": 290946\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "folder_rows = 0\n",
    "folder_above = 3000\n",
    "folder_tokens = 0\n",
    "file_breakdown: List[dict] = []\n",
    "paths = sorted(glob.glob(os.path.join(folder, 'clean_*.csv')))\n",
    "for p in paths:\n",
    "    rows = 0\n",
    "    toks = 0\n",
    "    above = 0\n",
    "    with open(p, 'r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            rows += 1\n",
    "            text = row.get('text') or ''\n",
    "            wc = len(text.split())\n",
    "            if wc >= int(min_words):\n",
    "                above += 1\n",
    "            token_est = row.get('token_est')\n",
    "            try:\n",
    "                toks += int(token_est) if token_est is not None else tok(text)\n",
    "            except Exception:\n",
    "                toks += tok(text)\n",
    "    folder_rows += rows\n",
    "    folder_above += above\n",
    "    folder_tokens += toks\n",
    "    file_breakdown.append({'file': p, 'rows': rows, 'above_min': above, 'tokens': int(toks)})\n",
    "\n",
    "summary = {\n",
    "    'folder': folder,\n",
    "    'files': len(paths),\n",
    "    'rows': folder_rows,\n",
    "    'above_min': folder_above,\n",
    "    'tokens': int(folder_tokens),\n",
    "}\n",
    "print(json.dumps(summary, indent=2))\n",
    "print('Per-file breakdown (first 10):')\n",
    "for rec in file_breakdown[:10]:\n",
    "    print(json.dumps(rec, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
